{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00cfaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install kagglehub nltk scikit-learn tensorflow pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344b11c",
   "metadata": {},
   "source": [
    "## Dataset Acquisition\n",
    "\n",
    "Loading sarcasm headlines dataset from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6295b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# Download dataset\n",
    "dataset_path = kagglehub.dataset_download(\"shariphthapa/sarcasm-json-datasets\")\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# Locate the JSON file\n",
    "json_candidates = [\n",
    "    os.path.join(dataset_path, \"Sarcasm.json\"),\n",
    "    os.path.join(dataset_path, \"Sarcasm_Headlines_Dataset.json\")\n",
    "]\n",
    "\n",
    "json_file = next((f for f in json_candidates if os.path.exists(f)), None)\n",
    "\n",
    "if not json_file:\n",
    "    raise FileNotFoundError(\"Dataset JSON file not found\")\n",
    "\n",
    "print(f\"Loading: {json_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eeef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore dataset\n",
    "try:\n",
    "    data = pd.read_json(json_file, lines=True)\n",
    "except ValueError:\n",
    "    data = pd.read_json(json_file)\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nColumns: {data.columns.tolist()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0964af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns and check class distribution\n",
    "data = data[['headline', 'is_sarcastic']].copy()\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(data['is_sarcastic'].value_counts())\n",
    "print(f\"\\nSarcastic ratio: {data['is_sarcastic'].mean():.2%}\")\n",
    "\n",
    "# Show sample headlines\n",
    "print(\"\\nSample headlines:\")\n",
    "for label in [0, 1]:\n",
    "    sample = data[data['is_sarcastic'] == label].iloc[0]['headline']\n",
    "    label_name = \"Sarcastic\" if label == 1 else \"Literal\"\n",
    "    print(f\"  [{label_name}] {sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e3ac0",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Cleaning and tokenizing headlines while removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Download NLTK data\n",
    "for resource in ['punkt', 'punkt_tab', 'stopwords']:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{resource}' if resource != 'stopwords' else 'corpora/stopwords')\n",
    "    except LookupError:\n",
    "        nltk.download(resource, quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"Loaded {len(stop_words)} English stop words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Normalize text: lowercase, remove punctuation, filter stop words\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words and empty tokens\n",
    "    tokens = [t for t in tokens if t and t not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['headline'].apply(clean_text)\n",
    "\n",
    "# Show preprocessing effect\n",
    "print(\"Preprocessing examples:\\n\")\n",
    "for idx in range(2):\n",
    "    print(f\"Original: {data['headline'].iloc[idx]}\")\n",
    "    print(f\"Cleaned:  {' '.join(data['tokens'].iloc[idx])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f328f5b",
   "metadata": {},
   "source": [
    "## Vectorization with Tokenizer\n",
    "\n",
    "Converting text to numerical sequences using vocabulary mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746700ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Configure tokenizer\n",
    "VOCAB_SIZE = 5000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<UNK>\")\n",
    "\n",
    "# Fit on cleaned text\n",
    "text_corpus = [' '.join(tokens) for tokens in data['tokens']]\n",
    "tokenizer.fit_on_texts(text_corpus)\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Using top {VOCAB_SIZE} words\")\n",
    "print(f\"\\nMost frequent words: {list(tokenizer.word_index.items())[:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc47240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to sequences and pad\n",
    "sequences = tokenizer.texts_to_sequences(text_corpus)\n",
    "\n",
    "# Determine sequence length\n",
    "seq_lengths = [len(s) for s in sequences]\n",
    "MAX_LENGTH = min(max(seq_lengths), 100)\n",
    "\n",
    "print(f\"Sequence length statistics:\")\n",
    "print(f\"  Mean: {np.mean(seq_lengths):.1f}\")\n",
    "print(f\"  Max: {max(seq_lengths)}\")\n",
    "print(f\"  Using: {MAX_LENGTH}\")\n",
    "\n",
    "# Pad sequences\n",
    "X_padded = pad_sequences(sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "print(f\"\\nPadded sequences shape: {X_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6ceb7",
   "metadata": {},
   "source": [
    "## Baseline Model: Logistic Regression\n",
    "\n",
    "Training on token indices as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5bc75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = X_padded.astype('float32')\n",
    "y = data['is_sarcastic'].values\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1903a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "baseline_model = LogisticRegression(\n",
    "    max_iter=3000, \n",
    "    random_state=42, \n",
    "    class_weight='balanced',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "print(\"Training baseline model...\")\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "y_proba_baseline = baseline_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "acc_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "auc_baseline = roc_auc_score(y_test, y_proba_baseline)\n",
    "\n",
    "print(f\"\\nBaseline Results:\")\n",
    "print(f\"  Accuracy: {acc_baseline:.4f}\")\n",
    "print(f\"  ROC AUC: {auc_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed baseline evaluation\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_baseline))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline, \n",
    "                          target_names=['Not Sarcastic', 'Sarcastic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683bd28d",
   "metadata": {},
   "source": [
    "## Enhanced Model: Embedding Layer\n",
    "\n",
    "Using learned word embeddings to capture semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ebc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Embedding configuration\n",
    "EMBEDDING_DIM = 64\n",
    "VOCAB_INPUT_SIZE = VOCAB_SIZE + 1  # Account for padding\n",
    "\n",
    "# Create embedding layer\n",
    "embedding_layer = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=VOCAB_INPUT_SIZE,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        input_length=MAX_LENGTH\n",
    "    )\n",
    "])\n",
    "\n",
    "print(f\"Embedding configuration:\")\n",
    "print(f\"  Vocabulary: {VOCAB_INPUT_SIZE}\")\n",
    "print(f\"  Dimensions: {EMBEDDING_DIM}\")\n",
    "print(f\"  Sequence length: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7698b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "print(\"Computing embeddings for all sequences...\")\n",
    "embeddings = embedding_layer.predict(X_padded, verbose=0)\n",
    "\n",
    "print(f\"Embedding tensor shape: {embeddings.shape}\")\n",
    "\n",
    "# Aggregate embeddings (average pooling)\n",
    "X_embedded = embeddings.mean(axis=1)\n",
    "\n",
    "print(f\"Aggregated embeddings shape: {X_embedded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split embedded data\n",
    "X_train_emb, X_test_emb, y_train_emb, y_test_emb = train_test_split(\n",
    "    X_embedded, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train on embeddings\n",
    "embedding_model = LogisticRegression(\n",
    "    max_iter=3000,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "print(\"Training embedding-based model...\")\n",
    "embedding_model.fit(X_train_emb, y_train_emb)\n",
    "\n",
    "# Predictions\n",
    "y_pred_emb = embedding_model.predict(X_test_emb)\n",
    "y_proba_emb = embedding_model.predict_proba(X_test_emb)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "acc_emb = accuracy_score(y_test_emb, y_pred_emb)\n",
    "auc_emb = roc_auc_score(y_test_emb, y_proba_emb)\n",
    "\n",
    "print(f\"\\nEmbedding Model Results:\")\n",
    "print(f\"  Accuracy: {acc_emb:.4f}\")\n",
    "print(f\"  ROC AUC: {auc_emb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed embedding model evaluation\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_emb, y_pred_emb))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_emb, y_pred_emb, \n",
    "                          target_names=['Not Sarcastic', 'Sarcastic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d9df44",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Analyzing performance differences between approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948015d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Baseline (Token IDs)', 'Embedding-based'],\n",
    "    'Accuracy': [acc_baseline, acc_emb],\n",
    "    'ROC AUC': [auc_baseline, auc_emb]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Calculate improvements\n",
    "acc_diff = (acc_emb - acc_baseline) * 100\n",
    "auc_diff = auc_emb - auc_baseline\n",
    "\n",
    "print(f\"\\nImprovement with embeddings:\")\n",
    "print(f\"  Accuracy: {acc_diff:+.2f} percentage points\")\n",
    "print(f\"  ROC AUC: {auc_diff:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c067b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "models = ['Baseline', 'Embedding']\n",
    "accuracies = [acc_baseline, acc_emb]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "axes[0].bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_ylim(0.5, 1.0)\n",
    "axes[0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "axes[0].legend()\n",
    "\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# ROC curves\n",
    "fpr_baseline, tpr_baseline, _ = roc_curve(y_test, y_proba_baseline)\n",
    "fpr_emb, tpr_emb, _ = roc_curve(y_test_emb, y_proba_emb)\n",
    "\n",
    "axes[1].plot(fpr_baseline, tpr_baseline, label=f'Baseline (AUC={auc_baseline:.4f})', \n",
    "            color=colors[0], linewidth=2)\n",
    "axes[1].plot(fpr_emb, tpr_emb, label=f'Embedding (AUC={auc_emb:.4f})', \n",
    "            color=colors[1], linewidth=2)\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb73e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on custom headlines\n",
    "def predict_sarcasm(headline, use_embedding=True):\n",
    "    \"\"\"Predict if a headline is sarcastic\"\"\"\n",
    "    # Preprocess\n",
    "    tokens = clean_text(headline)\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # Tokenize and pad\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, maxlen=MAX_LENGTH, padding='post')\n",
    "    \n",
    "    if use_embedding:\n",
    "        # Use embedding model\n",
    "        emb = embedding_layer.predict(padded, verbose=0).mean(axis=1)\n",
    "        prob = embedding_model.predict_proba(emb)[0, 1]\n",
    "        pred = embedding_model.predict(emb)[0]\n",
    "    else:\n",
    "        # Use baseline model\n",
    "        prob = baseline_model.predict_proba(padded)[0, 1]\n",
    "        pred = baseline_model.predict(padded)[0]\n",
    "    \n",
    "    return pred, prob\n",
    "\n",
    "# Test examples\n",
    "test_headlines = [\n",
    "    \"Local Man Wins Nobel Prize for Physics\",\n",
    "    \"Area Man Knows All The Shortcuts That Will Save You Time\",\n",
    "    \"Scientists Discover Cure for Major Disease\",\n",
    "    \"Nation's Girlfriends Unveil New Economic Plan: Let's Just Stay In Tonight\"\n",
    "]\n",
    "\n",
    "print(\"Testing custom headlines:\\n\")\n",
    "for headline in test_headlines:\n",
    "    pred, prob = predict_sarcasm(headline, use_embedding=True)\n",
    "    label = \"SARCASTIC\" if pred == 1 else \"LITERAL\"\n",
    "    print(f\"{label} ({prob:.2%}): {headline}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
